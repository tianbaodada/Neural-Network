{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from numpy.lib.npyio import load\n",
    "from dense import Dense\n",
    "from convolutional import Convolutional\n",
    "from maxpool2d import Maxpool2d\n",
    "from reshape import Reshape\n",
    "from activations import Sigmoid, Tanh, Relu, Softmax\n",
    "from losses import binary_cross_entropy, binary_cross_entropy_prime\n",
    "from network import train\n",
    "\n",
    "def preprocess_data(x, y, limit):\n",
    "    # zero_index = np.where(y == 0)[0][:limit]\n",
    "    # one_index = np.where(y == 1)[0][:limit]\n",
    "    all_indices = np.hstack(list(np.where(y == x)[0][:limit] for x in range(10)))\n",
    "    all_indices = np.random.permutation(all_indices)\n",
    "    x, y = x[all_indices], y[all_indices]\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    y = np_utils.to_categorical(y)\n",
    "    y = y.reshape(len(y), 10, 1)\n",
    "    return x, y\n",
    "\n",
    "# load MNIST from server, limit to 100 images per class since we're not training on GPU\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, y_train = preprocess_data(x_train, y_train, 100)\n",
    "x_valid, y_valid = x_train[900:], y_train[900:]\n",
    "x_train, y_train = x_train[:900], y_train[:900]\n",
    "x_test, y_test = preprocess_data(x_test, y_test, 100)\n",
    "\n",
    "l2 = 1e-1\n",
    "learning_rate = 3e-2\n",
    "\n",
    "# neural network\n",
    "network = [\n",
    "    Convolutional((1, 28, 28), 3, 10, l2=l2),\n",
    "    Maxpool2d(0, 2, 2, (10, 13, 13)),\n",
    "    Tanh(),\n",
    "    Convolutional((10, 13, 13), 3, 10, l2=l2),\n",
    "    Maxpool2d(1, 2, 2, (10, 6, 6)),\n",
    "    Tanh(),\n",
    "    Reshape((10, 6, 6), (10 * 6 * 6, 1)),\n",
    "    Dense(10 * 6 * 6, 100, l2=l2),\n",
    "    Tanh(),\n",
    "    Dense(100, 10, l2=l2),\n",
    "    Softmax()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Convolutional((1, 28, 28), 3, 10, l2=0.1)',\n",
       " 'Maxpool2d(0, 2, 2, (10, 13, 13))',\n",
       " 'Tanh',\n",
       " 'Convolutional((10, 13, 13), 3, 10, l2=0.1)',\n",
       " 'Maxpool2d(1, 2, 2, (10, 6, 6))',\n",
       " 'Tanh',\n",
       " 'Reshape((10, 6, 6), (360, 1))',\n",
       " 'Dense(360, 100, l2=0.1)',\n",
       " 'Tanh',\n",
       " 'Dense(100, 10, l2=0.1)',\n",
       " 'Softmax']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(layer) for layer in network]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af2ceb44aa90688c2930d6dd4501aa1bcab715860e6ec9fd29767ccc9bf88175"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('DeepLearning': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
